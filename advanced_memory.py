# advanced_memory.py - –î–æ–±–∞–≤–∏—Ç—å –≤ state.py

from datetime import datetime, timedelta
from collections import defaultdict
import logging
import numpy as np
from telegram import Update
from telegram.ext import ContextTypes

logger = logging.getLogger(__name__)

# ========== –í–ï–ö–¢–û–†–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ ========== #

class VectorMemory:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    """
    
    def __init__(self):
        self.memories = defaultdict(list)  # user_id -> [(text, embedding, timestamp, importance)]
        self.embedding_dim = 384  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_index = defaultdict(dict)  # user_id -> {memory_id: embedding}
    
    def _simple_embedding(self, text: str) -> list:
        """
        –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ real embeddings –æ—Ç Gemini).
        """
        # TF-IDF –ø–æ–¥–æ–±–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        words = text.lower().split()
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
        
        # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä –∏–∑ —Ç–æ–ø-—Å–ª–æ–≤
        vector = [0.0] * 100  # —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        for i, word in enumerate(sorted(word_freq.keys())[:100]):
            if i < 100:
                vector[i] = word_freq[word] / len(words)
        
        return vector
    
    def add_memory(self, user_id: int, text: str, importance: float = 0.5):
        """???????? ????????????."""
        embedding = self._simple_embedding(text)
        memory_id = len(self.memories[user_id])

        memory = {
            "id": memory_id,
            "text": text,
            "embedding": embedding,
            "timestamp": datetime.utcnow().isoformat(),
            "importance": importance,
            "access_count": 0
        }
        
        self.memories[user_id].append(memory)
        self.embedding_index[user_id][memory_id] = embedding
        
        # ???????????? ?????? ??????
        if len(self.memories[user_id]) > 1000:
            removed_ids = [m["id"] for m in self.memories[user_id][800:]]
            for rid in removed_ids:
                if rid in self.embedding_index[user_id]:
                    del self.embedding_index[user_id][rid]
            self.memories[user_id] = self.memories[user_id][:800]

    def cosine_similarity(self, vec1: list, vec2: list) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏."""
        if not vec1 or not vec2:
            return 0.0
        
        dot = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot / (norm1 * norm2)
    
    def search_similar(self, user_id: int, query: str, top_k: int = 5) -> list:
        """????? ??????? ????????????."""
        if user_id not in self.embedding_index:
            return []
        
        query_emb = self._simple_embedding(query)
        similarities = {}
        for mem_id, embedding in self.embedding_index[user_id].items():
            sim = self.cosine_similarity(query_emb, embedding)
            similarities[mem_id] = sim
        
        top_ids = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        results = []
        for mem_id, sim in top_ids:
            mem = self.memories[user_id][mem_id]
            results.append({
                "text": mem["text"],
                "similarity": sim,
                "timestamp": mem["timestamp"],
                "importance": mem["importance"],
            })
            mem["access_count"] += 1
        
        return results


# ========== –ö–û–ù–¢–ï–ö–°–¢–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ ========== #

class ContextualMemory:
    """
    –ü–∞–º—è—Ç—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: —Å–≤—è–∑—ã–≤–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –º–µ—Å—Ç—É, —ç–º–æ—Ü–∏—è–º.
    """
    
    def __init__(self):
        self.contexts = defaultdict(lambda: defaultdict(list))
        # user_id -> context_type -> [memories]
    
    def add_context(self, user_id: int, context_type: str, data: dict):
        """
        –î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ.
        context_type: "temporal", "emotional", "topical", "relational"
        """
        data["timestamp"] = datetime.utcnow().isoformat()
        self.contexts[user_id][context_type].append(data)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
        if len(self.contexts[user_id][context_type]) > 500:
            self.contexts[user_id][context_type] = self.contexts[user_id][context_type][-400:]
    
    def get_temporal_context(self, user_id: int, hours_back: int = 24) -> list:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤."""
        cutoff = datetime.utcnow() - timedelta(hours=hours_back)
        temporal = self.contexts[user_id].get("temporal", [])
        
        return [
            m for m in temporal
            if datetime.fromisoformat(m["timestamp"]) > cutoff
        ]
    
    def get_emotional_context(self, user_id: int, emotion: str) -> list:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —ç–º–æ—Ü–∏–∏."""
        emotional = self.contexts[user_id].get("emotional", [])
        return [m for m in emotional if m.get("emotion") == emotion]
    
    def get_topical_context(self, user_id: int, topic: str) -> list:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Ç–µ–º–µ."""
        topical = self.contexts[user_id].get("topical", [])
        return [m for m in topical if topic in m.get("topics", [])]


# ========== –≠–ü–ò–ó–û–î–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ ========== #

class EpisodicMemory:
    """
    –ü–∞–º—è—Ç—å –æ –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏—è—Ö/—ç–ø–∏–∑–æ–¥–∞—Ö –≤ –∂–∏–∑–Ω–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    
    def __init__(self):
        self.episodes = defaultdict(list)
    
    def create_episode(self, user_id: int, title: str, description: str, 
                      importance: float, tags: list):
        """–°–æ–∑–¥–∞—Ç—å —ç–ø–∏–∑–æ–¥."""
        episode = {
            "id": len(self.episodes[user_id]) + 1,
            "title": title,
            "description": description,
            "importance": importance,
            "tags": tags,
            "timestamp": datetime.utcnow().isoformat(),
            "related_conversations": [],
            "reflections": []
        }
        self.episodes[user_id].append(episode)
        return episode
    
    def link_conversation(self, user_id: int, episode_id: int, conversation_snippet: str):
        """–°–≤—è–∑–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä —Å —ç–ø–∏–∑–æ–¥–æ–º."""
        for ep in self.episodes[user_id]:
            if ep["id"] == episode_id:
                ep["related_conversations"].append({
                    "snippet": conversation_snippet,
                    "timestamp": datetime.utcnow().isoformat()
                })
    
    def add_reflection(self, user_id: int, episode_id: int, reflection: str):
        """–î–æ–±–∞–≤–∏—Ç—å —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∫ —ç–ø–∏–∑–æ–¥—É."""
        for ep in self.episodes[user_id]:
            if ep["id"] == episode_id:
                ep["reflections"].append({
                    "text": reflection,
                    "timestamp": datetime.utcnow().isoformat()
                })
    
    def get_important_episodes(self, user_id: int, min_importance: float = 0.7) -> list:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã."""
        return [ep for ep in self.episodes[user_id] if ep["importance"] >= min_importance]


# ========== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø ========== #

vector_memory = VectorMemory()
contextual_memory = ContextualMemory()
episodic_memory = EpisodicMemory()


def _detect_tone(text: str) -> float:
    """–õ—ë–≥–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è."""
    lower = (text or "").lower()
    positive = ["great", "good", "—Å–ø–∞—Å–∏–±–æ", "thanks", "–∫–ª–∞—Å—Å", "–æ—Ç–ª–∏—á–Ω–æ", "—Å—É–ø–µ—Ä"]
    negative = ["bad", "sad", "—É–∂–∞—Å", "–ø—Ä–æ–±–ª–µ–º–∞", "–ø–ª–æ—Ö–æ", "–∑–ª–æ–π", "angry"]
    score = 0
    for kw in positive:
        if kw in lower:
            score += 1
    for kw in negative:
        if kw in lower:
            score -= 1
    return float(max(-1.0, min(1.0, score * 0.2)))


def enhanced_add_observation(user_id: int, message_text: str, profile: dict | None = None):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –ø–∞–º—è—Ç—å—é."""
    try:
        if profile is None:
            try:
                from state import get_profile
                profile = get_profile(user_id)
            except Exception:
                profile = {}

        clean_text = message_text or ""
        importance = 0.5
        if any(tag in clean_text.lower() for tag in ["–≤–∞–∂–Ω–æ", "—Å—Ä–æ—á–Ω–æ", "–ø–æ–º–æ–≥–∏", "–ø—Ä–æ–±–ª–µ–º–∞"]):
            importance = 0.9

        vector_memory.add_memory(user_id, clean_text, importance)

        contextual_memory.add_context(user_id, "temporal", {
            "text": clean_text,
            "hour": datetime.utcnow().hour
        })

        tone = _detect_tone(clean_text)
        if abs(tone) > 0.3:
            emotion = "positive" if tone > 0 else "negative"
            contextual_memory.add_context(user_id, "emotional", {
                "text": clean_text,
                "emotion": emotion,
                "intensity": abs(tone)
            })

        patterns = profile.get("patterns", []) if isinstance(profile, dict) else []
        if patterns:
            contextual_memory.add_context(user_id, "topical", {
                "text": clean_text,
                "topics": patterns
            })

        if importance > 0.8:
            episodic_memory.create_episode(
                user_id,
                title=f"–í–∞–∂–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ {datetime.utcnow().strftime('%Y-%m-%d')}",
                description=clean_text[:200],
                importance=importance,
                tags=patterns,
            )
    except Exception:
        logger.exception("Advanced memory pipeline failed for user %s", user_id)


def build_enhanced_context(user_id: int, current_message: str) -> str:
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –ø–∞–º—è—Ç—å."""
    context_parts = []
    
    # –ë–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    try:
        from state import build_super_context
        base = build_super_context(user_id)
    except Exception:
        base = ""
    if base:
        context_parts.append(f"=== –ë–ê–ó–û–í–´–ô –ö–û–ù–¢–ï–ö–°–¢ ===\n{base}")
    
    # –ü–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã
    similar = vector_memory.search_similar(user_id, current_message, top_k=3)
    if similar:
        context_parts.append("\n=== –ü–û–•–û–ñ–ò–ï –ü–†–û–®–õ–´–ï –†–ê–ó–ì–û–í–û–†–´ ===")
        for i, mem in enumerate(similar, 1):
            context_parts.append(
                f"{i}. –°—Ö–æ–¥—Å—Ç–≤–æ: {mem['similarity']:.2f}\n"
                f"   {mem['text'][:150]}..."
            )
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞)
    temporal = contextual_memory.get_temporal_context(user_id, hours_back=24)
    if temporal:
        context_parts.append(f"\n=== –ü–û–°–õ–ï–î–ù–ò–ï 24 –ß–ê–°–ê ===")
        context_parts.append(f"–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {len(temporal)} —Å–æ–±—ã—Ç–∏–π")
    
    # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    current_tone = _detect_tone(current_message)
    if abs(current_tone) > 0.3:
        emotion = "positive" if current_tone > 0 else "negative"
        emotional = contextual_memory.get_emotional_context(user_id, emotion)
        if emotional:
            context_parts.append(
                f"\n=== –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ ({emotion}) ===\n"
                f"–ü–æ—Ö–æ–∂–∏—Ö —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤: {len(emotional)}"
            )
    
    # –í–∞–∂–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã
    episodes = episodic_memory.get_important_episodes(user_id)
    if episodes:
        context_parts.append(f"\n=== –í–ê–ñ–ù–´–ï –≠–ü–ò–ó–û–î–´ ===")
        for ep in episodes[-3:]:
            context_parts.append(
                f"‚Ä¢ {ep['title']} (–≤–∞–∂–Ω–æ—Å—Ç—å: {ep['importance']:.2f})\n"
                f"  {ep['description'][:100]}..."
            )
    
    if not context_parts and base:
        return base
    return "\n\n".join(context_parts)


# –ö–æ–º–∞–Ω–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –ø–∞–º—è—Ç—å—é

async def search_memory(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É."""
    user_id = update.effective_user.id
    
    if not context.args:
        await update.message.reply_text("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /search <–∑–∞–ø—Ä–æ—Å>")
        return
    
    query = " ".join(context.args)
    results = vector_memory.search_similar(user_id, query, top_k=5)
    
    if not results:
        await update.message.reply_text("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–º—è—Ç–∏.")
        return
    
    text = f"üîç –ù–∞–π–¥–µ–Ω–æ –≤ –ø–∞–º—è—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n\n"
    for i, res in enumerate(results, 1):
        text += (
            f"{i}. –°—Ö–æ–¥—Å—Ç–≤–æ: {res['similarity']:.2f}\n"
            f"   {res['text'][:150]}...\n"
            f"   ({res['timestamp'][:10]})\n\n"
        )
    
    await update.message.reply_text(text)


async def show_episodes(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–∫–∞–∑–∞—Ç—å –≤–∞–∂–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã."""
    user_id = update.effective_user.id
    episodes = episodic_memory.get_important_episodes(user_id)
    
    if not episodes:
        await update.message.reply_text("–ü–æ–∫–∞ –Ω–µ—Ç –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤.")
        return
    
    text = "üìñ –í–∞–∂–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã:\n\n"
    for ep in episodes[-5:]:
        text += (
            f"‚Ä¢ {ep['title']} (‚≠ê {ep['importance']:.1f})\n"
            f"  {ep['description'][:100]}...\n"
            f"  –¢–µ–≥–∏: {', '.join(ep['tags'])}\n"
            f"  {ep['timestamp'][:10]}\n\n"
        )
    
    await update.message.reply_text(text)


async def create_episode(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–°–æ–∑–¥–∞—Ç—å —ç–ø–∏–∑–æ–¥ –≤—Ä—É—á–Ω—É—é."""
    user_id = update.effective_user.id
    
    if not context.args:
        await update.message.reply_text(
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /episode <–∑–∞–≥–æ–ª–æ–≤–æ–∫> | <–æ–ø–∏—Å–∞–Ω–∏–µ> | <–≤–∞–∂–Ω–æ—Å—Ç—å 0-1> | <—Ç–µ–≥–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é>"
        )
        return
    
    text = " ".join(context.args)
    parts = text.split("|")
    
    if len(parts) < 4:
        await update.message.reply_text("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π | –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å.")
        return
    
    title = parts[0].strip()
    description = parts[1].strip()
    try:
        importance = float(parts[2].strip())
    except:
        importance = 0.5
    tags = [t.strip() for t in parts[3].split(",")]
    
    episode = episodic_memory.create_episode(user_id, title, description, importance, tags)
    
    await update.message.reply_text(
        f"‚úÖ –≠–ø–∏–∑–æ–¥ —Å–æ–∑–¥–∞–Ω:\n"
        f"ID: {episode['id']}\n"
        f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n"
        f"–í–∞–∂–Ω–æ—Å—Ç—å: {importance:.2f}"
    )
